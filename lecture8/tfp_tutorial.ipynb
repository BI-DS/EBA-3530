{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Introduction to TensorFlow Probability (TFP)\n",
    "## Modeling a Unimodal Gaussian Distribution\n",
    "\n",
    "**TensorFlow Probability (TFP)** is a library built on top of TensorFlow that makes it easy to combine probabilistic models with deep learning.\n",
    "\n",
    "In this notebook, we will explore the `tfp.distributions` layer (often aliased as `tfd`). We will focus on the simplest building block: the **Unimodal Gaussian (Normal) Distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import seaborn as sns\n",
    "\n",
    "# Common aliases\n",
    "tfd = tfp.distributions\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow Probability version:\", tfp.__version__)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining a Distribution\n",
    "\n",
    "In TFP, distributions are objects. We can create a Gaussian distribution by specifying its mean (`loc`) and standard deviation (`scale`).\n",
    "\n",
    "Let's create a standard normal distribution $\\mathcal{N}(\\mu=0, \\sigma=1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Normal distribution object\n",
    "# loc = mean, scale = standard deviation\n",
    "normal_dist = tfd.Normal(loc=0., scale=1.)\n",
    "\n",
    "print(normal_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sampling and Visualization\n",
    "\n",
    "Once we have a distribution object, we can perform two main operations:\n",
    "1.  **Sampling:** Generating random numbers from the distribution.\n",
    "2.  **Log Probability:** Calculating how likely a given value is under the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 data points from our standard normal distribution\n",
    "samples = normal_dist.sample(1000)\n",
    "\n",
    "# Evaluate the PDF at x (for plotting theory)\n",
    "x_values = np.linspace(-4, 4, 100)\n",
    "log_pdf_values = normal_dist.log_prob(x_values)\n",
    "pdf_values = tf.exp(log_pdf_values)\n",
    "\n",
    "# Visualizing the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histogram of samples\n",
    "sns.histplot(samples, kde=False, stat=\"density\", label=\"Samples (Histogram)\", color=\"skyblue\")\n",
    "\n",
    "# Plot the theoretical PDF\n",
    "plt.plot(x_values, pdf_values, 'r-', lw=3, label=\"Theoretical PDF\")\n",
    "\n",
    "plt.title(\"Standard Normal Distribution: Samples vs True PDF\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "This is where TFP shines. We can use TensorFlow's optimization tools to **learn** the parameters of a distribution from data.\n",
    "\n",
    "**The Scenario:** Imagine we observed some data `observed_data` (generated below), but we don't know the mean or standard deviation that produced it. We want to find the best $\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A. Generate \"Mystery\" Data ---\n",
    "\n",
    "# True parameters (hidden from our model)\n",
    "TRUE_MEAN = 5.0\n",
    "TRUE_STD = 2.0\n",
    "\n",
    "# Generate 200 observations\n",
    "obs_dist = tfd.Normal(loc=TRUE_MEAN, scale=TRUE_STD)\n",
    "observed_data = obs_dist.sample(200)\n",
    "\n",
    "print(f\"Observed data mean: {tf.reduce_mean(observed_data):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two variables for the parameters we want to learn.\n",
    "* `mu`: The mean (can be any real number).\n",
    "* `sigma_param`: The parameter controlling standard deviation. *Note: We use a softplus transformation later to ensure the standard deviation is always positive.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B. Define Variables ---\n",
    "\n",
    "# Initialize guesses (e.g., guessing mean=0, std=1)\n",
    "mu_var = tf.Variable(0.0, name=\"mean\")\n",
    "sigma_param_var = tf.Variable(0.0, name=\"sigma_param\") \n",
    "\n",
    "# --- C. Define Loss (Negative Log Likelihood) ---\n",
    "\n",
    "def nll_loss():\n",
    "    # 1. Transform the raw variable to a positive scale for Std Dev\n",
    "    current_sigma = tf.math.softplus(sigma_param_var)\n",
    "    \n",
    "    # 2. Define the distribution using current estimates\n",
    "    current_dist = tfd.Normal(loc=mu_var, scale=current_sigma)\n",
    "    \n",
    "    # 3. Calculate log_prob of the OBSERVED data\n",
    "    log_probs = current_dist.log_prob(observed_data)\n",
    "    \n",
    "    # 4. Sum them up and negate (to minimize)\n",
    "    return -tf.reduce_sum(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the training loop using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- D. Optimization Loop ---\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "print(\"--- Starting Optimization ---\")\n",
    "\n",
    "for i in range(200):\n",
    "    optimizer.minimize(nll_loss, var_list=[mu_var, sigma_param_var])\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        current_loss = nll_loss().numpy()\n",
    "        current_mu = mu_var.numpy()\n",
    "        current_std = tf.math.softplus(sigma_param_var).numpy()\n",
    "        print(f\"Step {i}: Loss = {current_loss:.1f}, Mean = {current_mu:.2f}, Std = {current_std:.2f}\")\n",
    "\n",
    "print(\"\\n--- Optimization Finished ---\")\n",
    "print(f\"True Mean: {TRUE_MEAN} | Learned Mean: {mu_var.numpy():.2f}\")\n",
    "print(f\"True Std:  {TRUE_STD} | Learned Std:  {tf.math.softplus(sigma_param_var).numpy():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final Visualization\n",
    "Let's see if the learned distribution matches the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mu = mu_var.numpy()\n",
    "final_std = tf.math.softplus(sigma_param_var).numpy()\n",
    "\n",
    "# Create the learned distribution\n",
    "learned_dist = tfd.Normal(loc=final_mu, scale=final_std)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 1. The Data\n",
    "sns.histplot(observed_data, kde=False, stat=\"density\", label=\"Observed Data\", color=\"gray\", alpha=0.5)\n",
    "\n",
    "# 2. The True Generator (Green dashed)\n",
    "x_range = np.linspace(min(observed_data), max(observed_data), 200)\n",
    "plt.plot(x_range, obs_dist.prob(x_range), 'g--', lw=2, label=f\"True Generator (Mean={TRUE_MEAN})\")\n",
    "\n",
    "# 3. The Learned Model (Red solid)\n",
    "plt.plot(x_range, learned_dist.prob(x_range), 'r-', lw=3, label=f\"Learned Model (Mean={final_mu:.2f})\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Maximum Likelihood Estimation with TFP\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
