{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sequential_and_lasso.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOlzQEH/sWb49ONPTeYFUiC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":204,"metadata":{"id":"3efg9jqTOHDA","executionInfo":{"status":"ok","timestamp":1645528351654,"user_tz":-60,"elapsed":432,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","from sklearn.feature_selection import SequentialFeatureSelector as sfs\n","from sklearn.linear_model import LinearRegression\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# Sequential feature selection and Lasso "],"metadata":{"id":"8z4twNROYBpg"}},{"cell_type":"code","source":["# Number of observations\n","N = 500\n","\n","# Numbers of predictors\n","p = 10\n","\n","# True variance in regression\n","sigma2 = 1\n","\n","# Generate some random x's\n","x = np.random.normal(size=(N,p))\n","# give some names\n","names = pd.DataFrame(['feature_'+str(i) for i in range(p)])\n","\n","# Define the relevant (true) x's\n","true_x_idx = [0, 1, 2, 3]\n","true_B = np.array([0.5, 1, -5, 5]).reshape((1,4))\n","\n","# Generate y \n","y = np.dot(x[:,true_x_idx],np.transpose(true_B)) + \\\n","              np.random.normal(size=(N,1))*np.sqrt(sigma2)"],"metadata":{"id":"1iWdluBiOdBr","executionInfo":{"status":"ok","timestamp":1645531880361,"user_tz":-60,"elapsed":12,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}}},"execution_count":278,"outputs":[]},{"cell_type":"markdown","source":["## Forward feature selection\n","It is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero feature and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the `n_features_to_select` parameter.\n"],"metadata":{"id":"i4zUSjbNX5jU"}},{"cell_type":"code","source":["linear_base = LinearRegression()\n","forward_selection = sfs(estimator=linear_base, n_features_to_select=6,\\\n","                direction='forward', scoring='neg_mean_squared_error').fit(x,y)\n","print(\"Features selected by forward sequential selection: {}\".\\\n","      format(names[forward_selection.get_support()]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Ksvw434PYBZ","executionInfo":{"status":"ok","timestamp":1645531886304,"user_tz":-60,"elapsed":428,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}},"outputId":"4bd11ae7-0cd4-4231-950b-157124f2ecce"},"execution_count":279,"outputs":[{"output_type":"stream","name":"stdout","text":["Features selected by forward sequential selection:            0\n","0  feature_0\n","1  feature_1\n","2  feature_2\n","3  feature_3\n","6  feature_6\n","8  feature_8\n"]}]},{"cell_type":"markdown","source":["Now use the chosen features and fit a OLS model using the `statsmodels` library."],"metadata":{"id":"AaUuaCrVmjFA"}},{"cell_type":"code","source":["# only the features selected by the forward approach\n","x_fs = x[:,forward_selection.get_support()]\n","forward_lr = sm.OLS(y,sm.add_constant(x_fs)).fit()\n","print(forward_lr.summary())\n"],"metadata":{"id":"DDOpXkkdhHLH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Backward feature selection\n","It follows the same idea as the forward approach, but works in the opposite direction: instead of starting with no feature and greedily adding features, it starts with all the features and greedily remove features from the set. The `direction` parameter in the function controls whether forward or backward is used.\n","\n"],"metadata":{"id":"8aKDgPjKYIkq"}},{"cell_type":"code","source":["backward_selection = sfs(estimator=linear_base, n_features_to_select=6,\\\n","                direction='backward', scoring='neg_mean_squared_error').fit(x,y)\n","print(\"Features selected by backward sequential selection: {}\".\\\n","      format(names[backward_selection.get_support()]))"],"metadata":{"id":"hUKLmC8yTXTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# only the features selected by the backward approach\n","x_bs = x[:,backward_selection.get_support()]\n","backward_lr = sm.OLS(y,sm.add_constant(x_bs)).fit()\n","print(backward_lr.summary())"],"metadata":{"id":"6UNkitsdUU_F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** In general, forward and backward selection do not yield equivalent results. Also, one may be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3."],"metadata":{"id":"F7AY_R5lnG64"}},{"cell_type":"markdown","source":["## Least Absolute Shrinkage and Selection Operator (LASSO)"],"metadata":{"id":"z_s82EfznzXK"}},{"cell_type":"code","source":["# For LASSO, select the penalty term L1_wt to be 1 \n","from sklearn.linear_model import Lasso\n","lassoreg = Lasso().fit(x,y)\n","lasso_coeff = []\n","lasso_coeff.extend(lassoreg.intercept_)\n","lasso_coeff.extend(lassoreg.coef_[lassoreg.coef_!=0])\n","print(\"Features selected by LASSO:\\n{}\\nwith coefficients {}\".\\\n","      format('intercept\\n'+names[lassoreg.coef_!=0].to_string(index=False,\\\n","            header=False), np.round(lasso_coeff,4)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZJ8hJuhfphq","executionInfo":{"status":"ok","timestamp":1645531908131,"user_tz":-60,"elapsed":427,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}},"outputId":"1a57878a-8e6c-45d9-ffc4-b0e4ed30f01e"},"execution_count":283,"outputs":[{"output_type":"stream","name":"stdout","text":["Features selected by LASSO:\n","intercept\n","feature_1\n","feature_2\n","feature_3\n","with coefficients [-0.1912  0.0985 -4.079   3.8815]\n"]}]},{"cell_type":"markdown","source":["### Cross-valitation with LASSO"],"metadata":{"id":"367ikETs7KBp"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","\n","# Choose different values for the hyperparameter alpha\n","alphas = [1e-15, 1e-10, 1e-8, 1e-5, 1e-4, 1e-3, 1e-2, 1, 5, 10]\n","\n","# for each alpha value, we conduct a 5-folds CV\n","cv = 5\n","\n","# save mean squared errors in a matrix\n","all_scores = np.zeros((len(alphas),cv))\n","\n","for i,alpha in enumerate(alphas):\n","  kfolds = KFold(n_splits=cv, shuffle=True)\n","  all_scores[i,:] = cross_val_score(Lasso(alpha=alpha),x,y,cv=kfolds,\\\n","                           scoring='neg_mean_squared_error')\n","\n","print('Average mse {} with std {}'.\\\n","      format(np.round(-1*np.mean(all_scores,axis=1),4),\\\n","             np.round(np.std(all_scores,axis=1),4)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qa6MPS61ZqS","executionInfo":{"status":"ok","timestamp":1645531947790,"user_tz":-60,"elapsed":503,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}},"outputId":"296bad2d-efd1-4b69-f34f-e5ee98282888"},"execution_count":285,"outputs":[{"output_type":"stream","name":"stdout","text":["Average mse [ 1.0296  1.0406  1.0416  1.0289  1.0555  1.0499  1.0316  4.4017 50.776\n"," 52.7242] with std [0.1085 0.1081 0.0472 0.1077 0.1954 0.1545 0.1333 1.3534 3.9181 4.4862]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"5cxurY5QNTfW","executionInfo":{"status":"ok","timestamp":1645529629847,"user_tz":-60,"elapsed":478,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}}},"execution_count":230,"outputs":[]},{"cell_type":"code","source":["avg_mse = -1*np.mean(all_scores,axis=1)\n","std_mse = np.std(all_scores,axis=1)\n","plt.errorbar(range(10),avg_mse, yerr=std_mse, fmt='.', color='r')\n","plt.xticks(range(10), [str(a) for a in alphas])\n","plt.xlabel('alpha value')\n","plt.ylabel('mse')\n","plt.show()"],"metadata":{"id":"6aonT-aXI5lX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Effect of the hyperparameter Î» on $\\beta$"],"metadata":{"id":"IfAwSxO7XoVt"}},{"cell_type":"code","source":["# create a LASSO model for different alpha values\n","# and fit the model\n","lasso_1 =  Lasso(alpha=alphas[0]).fit(x,y)\n","lasso_2 =  Lasso(alpha=alphas[5]).fit(x,y)\n","lasso_3 =  Lasso(alpha=alphas[7]).fit(x,y)\n","# get predicted values based on the fitted model\n","y_pred1 = lasso_1.predict(x)\n","y_pred2 = lasso_2.predict(x)\n","y_pred3 = lasso_3.predict(x)\n","# get sum of squared residuals (rss)\n","rss1 = np.round(np.sum((y_pred1 -y)**2),1)\n","rss2 = np.round(np.sum((y_pred2 -y)**2),1)\n","rss3 = np.round(np.sum((y_pred3 -y)**2),1)\n","\n","# create a list with: i) rss and ii) model coefficients\n","results1 = [rss1]\n","results1.extend(np.round(lasso_1.intercept_.tolist(),3))\n","results1.extend(np.round(lasso_1.coef_.tolist(),3))\n","results2 = [rss2]\n","results2.extend(np.round(lasso_2.intercept_.tolist(),3))\n","results2.extend(np.round(lasso_2.coef_.tolist(),3))\n","results3 = [rss3]\n","results3.extend(np.round(lasso_3.intercept_.tolist(),3))\n","results3.extend(np.round(lasso_3.coef_.tolist(),3))\n","\n","# create a data frame for easy visualization of retults\n","col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(p)]\n","ind = ['alpha_%.2g'%alphas[i] for i in [0,5,7]]\n","matrix_lasso = pd.DataFrame(index=ind, columns=col)\n","\n","# just append the results row-wise\n","matrix_lasso.iloc[0,:] = results1\n","matrix_lasso.iloc[1,:] = results2\n","matrix_lasso.iloc[2,:] = results3\n","# save as csv. It is easier to see all coefficients\n","matrix_lasso.to_csv('lasso_coeff.csv', sep=';')\n","print(matrix_lasso)\n"],"metadata":{"id":"runItRG3JeTK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Alternatively use `LassoCV` that saves the best model in the CV."],"metadata":{"id":"8-kcLzvln66U"}},{"cell_type":"code","source":["from sklearn.linear_model import LassoCV\n","\n","# define the LassoCV object\n","Lasso = LassoCV(cv=cv, fit_intercept=True).fit(x,y.ravel())\n","print('The optimal alpha is {:.4f}:'.format(Lasso.alpha_))\n","\n","# compute the path\n","[alphas,weights,c] = Lasso.path(x, y.ravel())\n","\n","# plot coeffs as a function of alpha\n","fig = plt.figure()\n","plt.plot(alphas, weights.T)\n","plt.show()"],"metadata":{"id":"tREtDA-6iHrd"},"execution_count":null,"outputs":[]}]}