{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multicollinearity.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOU+EK7LnN0dZUOTR4gcWD3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mj5vWffrrd0v"},"outputs":[],"source":["import numpy as np\n","import statsmodels.api as sm"]},{"cell_type":"markdown","source":["# Multicollinearity\n","Let's simulate some data to see the effect of multicollinearity and how can we solve a wrongly specified design matrix. "],"metadata":{"id":"cYW_5saA1W6P"}},{"cell_type":"code","source":["# Number of observations for each category\n","Nk = 30\n","\n","# Number of categories\n","K = 3\n","\n","# Total number of observations\n","N = Nk*K\n","\n","#% Construct a x matrix with dummy variables (zero one vectors). \n","x = np.zeros((N, K + 1)) # N obversvations in total. K categories & intercept\n","x[:,0] = np.ones(N) # Add the intercept\n","\n","# Loop in the dummies\n","xo = np.ones(Nk)\n","cnt = 0\n","\n","# the first column is the intercept in the regression\n","# the second column a dummy variable for the first category, etc. \n","for i in range(K):\n","  x[cnt:Nk+cnt,i+1] = xo\n","  cnt = cnt + Nk"],"metadata":{"id":"VIk0Ll5IriSt","executionInfo":{"status":"ok","timestamp":1645554762978,"user_tz":-60,"elapsed":15,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Let y be a function of some random noise + intercept. AND, let the true\n","# dummy for the thirs category be zero. This means that the first category\n","# has the same intercept as the intercept itself. \n","\n","beta = np.array([10, 5, -5, 0])\n","# y = x'*beta will give\n","# y_1 = 10 + 5  = 15 for category 1\n","# y_2 = 10 - 5 = 5   for category 2\n","# y_3 = 10     = 10  for category 3\n","# But we also add some noise\n","y = np.dot(x,beta) + np.random.normal(size=(N))"],"metadata":{"id":"LkQ6en8Rtqt5","executionInfo":{"status":"ok","timestamp":1645555208984,"user_tz":-60,"elapsed":316,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["model = sm.OLS(y,x).fit()\n","print(model.summary())"],"metadata":{"id":"2OpqwfNIsUeo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**See the warning message!** The regression is rank deficient,\n","mening that the variables in x are linear combination of each other. In particular, we know from above that `np.sum(x[:,1:],1)=1`, i.e., all columns sum to 1, which is the same as `x[:,0]` (the intercept)\n"],"metadata":{"id":"S87dhcRSvfWI"}},{"cell_type":"markdown","source":["**Dropping the 1st category**"],"metadata":{"id":"b4HBYbyWwfVs"}},{"cell_type":"code","source":["model = sm.OLS(y,x[:,[0,2,3]]).fit()\n","print(model.summary())"],"metadata":{"id":"-oUfCakpwqEO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Remember the true beta is `[10 5 -5 0]` for intercept, cat1, cat2, and cat3. Notice that the estimated parameters for cat2 and cat3 are relative to cat1, i.e. beta[2] - beta[1] = -10 and beta[4] - beta[1] = -5. **Just as the estimated coefficients above!**"],"metadata":{"id":"pahu4t4bxgmK"}},{"cell_type":"markdown","source":["We know that $y = x'\\beta$, hence\n","* $y_1 = 10 + 5 \\rightarrow 15$\n","* $y_2 = 10 -5 \\rightarrow 5$\n","* $y_3 = 10  \\rightarrow 10$\n","\n","Let's double check..."],"metadata":{"id":"vl2B2Uemzqg5"}},{"cell_type":"code","source":["print('Estimate category 1 is {:.1f}'.format(model.params[0]))\n","print('Estimate category 2 is {:.1f}'.format(model.params[0]+model.params[1]))\n","print('Estimate category 3 is {:.1f}'.format(model.params[0]+model.params[2]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_exD7Sqwusk","executionInfo":{"status":"ok","timestamp":1645556817445,"user_tz":-60,"elapsed":18,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}},"outputId":"86d673a0-2f77-45fa-c994-bd8d7cc46e19"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Estimate category 1 is 15.2\n","Estimate category 2 is 4.7\n","Estimate category 3 is 9.9\n"]}]},{"cell_type":"markdown","source":["Now try a **lasso** model"],"metadata":{"id":"bOuCGBEv2IS7"}},{"cell_type":"code","source":["from sklearn.linear_model import LassoCV\n","\n","# define the LassoCV object\n","Lasso = LassoCV(cv=5, fit_intercept=True).fit(x,y.ravel())"],"metadata":{"id":"EHg9yWH30YnV","executionInfo":{"status":"ok","timestamp":1645557028936,"user_tz":-60,"elapsed":669,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["Print lasso coefficients ..."],"metadata":{"id":"P52Lhd6f23vy"}},{"cell_type":"code","source":["print('Lasso estimated intercept is {:.1f}'.format(Lasso.intercept_))\n","print('Lasso estimated coefficients are {}'.format(np.round(Lasso.coef_[1:],1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"diKfXDTG15NL","executionInfo":{"status":"ok","timestamp":1645557226051,"user_tz":-60,"elapsed":398,"user":{"displayName":"rogelio andrade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXqySB_E_aFOyj-1DEnOzttJa3RmMvZHhdQETKvw=s64","userId":"17933401457496596047"}},"outputId":"ceb8a36a-a1db-41de-e3d9-10cb875a95ac"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Lasso estimated intercept is 9.9\n","Lasso estimated coefficients are [ 5.2 -5.1  0. ]\n"]}]}]}